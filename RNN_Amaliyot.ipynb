{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1-vazifa: Ma’lumotlarni tayyorlash\n",
        "\n",
        "maktab so‘zi uchun ma’lumotlarni tayyorlang.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"maktab\" so‘zini sequence o‘zgaruvchisiga yuklang.\n",
        "\n",
        "\n",
        "\n",
        "So‘zdan unikal harflar lug‘atini (chars) yarating.<br>\n",
        "\n",
        "\n",
        "\n",
        "Harflarni indeksga (char2idx) va indekslarni harfga (idx2char) o‘giruvchi lug‘atlarni yarating.\n",
        "\n",
        "\n",
        "\n",
        "Kiruvchi ma’lumot (x_data) va kutilayotgan natija (y_data) uchun ro‘yxatlarni yarating. Eslab qoling, x_data oxirgi harfsiz, y_data esa birinchi harfsiz bo‘lishi kerak.\n",
        "\n",
        "\n",
        "\n",
        "Tayyorlangan x_data va y_data ni tensorlarga aylantiring va unsqueeze() yordamida shaklini o‘zgartiring.\n",
        "\n",
        "\n",
        "\n",
        "2-vazifa: Modelni yaratish\n",
        "\n",
        "Videoda ko‘rsatilgan HarfRNN klassini yarating. Model arxitekturasi (RNN va Chiziqli qatlamlar) o‘zgarishsiz qolishi kerak. Faqat yangi vocab_size ga moslashishini ta’minlang.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nn.Module dan meros oluvchi HarfRNN klassini yarating.\n",
        "\n",
        "\n",
        "\n",
        "__init__ metodida nn.RNN va nn.Linear qatlamlarini e’lon qiling.\n",
        "\n",
        "\n",
        "\n",
        "forward metodini yozing, u x va hidden holatlarini qabul qilib, natija (out) va yangi hidden holatini qaytarsin.\n",
        "\n",
        "\n",
        "\n",
        "3-vazifa: Modelni o‘qitish\n",
        "\n",
        "Modelni yangi giperparametrlar bilan o‘qiting.<br><br>1. hidden_size ni 10 ga, lr (o‘rganish tezligi) ni 0.02 ga va epochs (epoxalar) sonini 200 ga o‘rnating.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "hidden_size ni 10 ga, lr (o‘rganish tezligi) ni 0.02 ga va epochs (epoxalar) sonini 200 ga o‘rnating.\n",
        "\n",
        "\n",
        "\n",
        "Model, yo‘qotish funksiyasi (criterion), va optimizatorni (optimizer) e’lon qiling.\n",
        "\n",
        "\n",
        "\n",
        "O‘qitish siklini yarating. Sikl ichida har bir element uchun yo‘qotishni hisoblang, gradientlarni nollang, loss.backward() va optimizer.step() ni chaqiring.\n",
        "\n",
        "\n",
        "\n",
        "Har 20 epoxada yo‘qotish qiymati va model bashorat qilgan ketma-ketlikni (pred_seq) konsolga chiqaring.\n",
        "\n",
        "\n",
        "\n",
        "4-vazifa: Natijalarni tahlil qilish\n",
        "\n",
        "3-vazifada olingan natijalarga qarab quyidagi savolga javob bering: <br><br>\"Model maktab so‘zining keyingi harflarini to‘g‘ri bashorat qilishni o‘rgandimi? Javobingizni epoxalar davomida yo‘qotish (Loss) qiymatining o‘zgarishi va oxirgi bashorat (Pred) natijasiga tayanib asoslang.\""
      ],
      "metadata": {
        "id": "sIzugSl3A_IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "oylnUrhO_ars"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op43Rlnl9gdL",
        "outputId": "a53278e8-7423-46ae-b64d-47c6eef41b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original indekslar: [0, 2, 4, 3, 2, 1]\n",
            "x_data (input): [0, 2, 4, 3, 2]\n",
            "y_data (target): [2, 4, 3, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "# 1\n",
        "\n",
        "sequence = \"maktab\"\n",
        "chars = set(sequence)\n",
        "\n",
        "char2idx = {}\n",
        "idx2char = {}\n",
        "\n",
        "for char in chars:\n",
        "    char2idx[char] = len(char2idx)\n",
        "    idx2char[len(idx2char)] = char\n",
        "\n",
        "print(char2idx)\n",
        "print(idx2char)\n",
        "\n",
        "input_indices = [char2idx[ch] for ch in sequence]\n",
        "print(input_indices)\n",
        "\n",
        "x_data = input_indices[:-1]\n",
        "\n",
        "y_data = input_indices[1:]\n",
        "\n",
        "print(f\"Original indekslar: {input_indices}\")\n",
        "print(f\"x_data (input): {x_data}\")\n",
        "print(f\"y_data (target): {y_data}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1\n",
        "\n",
        "x_tensor = torch.LongTensor(x_data)\n",
        "y_tensor = torch.LongTensor(y_data)\n",
        "\n",
        "x_tensor = x_tensor.unsqueeze(0)\n",
        "y_tensor = y_tensor.unsqueeze(0)\n",
        "\n",
        "print(f\"x_tensor shakli: {x_tensor.shape}\")\n",
        "print(f\"y_tensor shakli: {y_tensor.shape}\")\n",
        "\n",
        "x_tensor = torch.LongTensor(x_data)\n",
        "y_tensor = torch.LongTensor(y_data)\n",
        "\n",
        "x_tensor = x_tensor.unsqueeze(0)\n",
        "y_tensor = y_tensor.unsqueeze(0)\n",
        "\n",
        "print(f\"x_tensor shakli: {x_tensor.shape}\")\n",
        "print(f\"y_tensor shakli: {y_tensor.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5yu9BQi98hi",
        "outputId": "6b1e1af9-ab25-4ab9-ce41-a07351c35c50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_tensor shakli: torch.Size([1, 5])\n",
            "y_tensor shakli: torch.Size([1, 5])\n",
            "x_tensor shakli: torch.Size([1, 5])\n",
            "y_tensor shakli: torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2\n",
        "\n",
        "class HarfRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size):\n",
        "        super(HarfRNN, self).__init__()\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=input_size,\n",
        "                          hidden_size=hidden_size,\n",
        "                          batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "-BQ946W2_gRS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3\n",
        "\n",
        "\n",
        "hidden_size = 10\n",
        "lr = 0.02\n",
        "epochs = 200\n",
        "input_size = len(char2idx)\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "x_one_hot = torch.eye(vocab_size)[x_tensor]\n",
        "\n",
        "model = HarfRNN(vocab_size, input_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    hidden = torch.zeros(1, 1, hidden_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs, hidden = model(x_one_hot, hidden)\n",
        "\n",
        "    loss = criterion(outputs.view(-1, vocab_size), y_tensor.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        result = outputs.data.numpy().argmax(axis=2)\n",
        "        pred_str = ''.join([idx2char[c] for c in result.flatten()])\n",
        "\n",
        "        print(f\"Epoch: {epoch}/{epochs}, Loss: {loss.item():.4f}, Prediction: {pred_str}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdopgjfQ__p8",
        "outputId": "5742260e-0364-4c04-95db-f2046b874953"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20/200, Loss: 0.1961, Prediction: aktab\n",
            "Epoch: 40/200, Loss: 0.0150, Prediction: aktab\n",
            "Epoch: 60/200, Loss: 0.0065, Prediction: aktab\n",
            "Epoch: 80/200, Loss: 0.0045, Prediction: aktab\n",
            "Epoch: 100/200, Loss: 0.0035, Prediction: aktab\n",
            "Epoch: 120/200, Loss: 0.0029, Prediction: aktab\n",
            "Epoch: 140/200, Loss: 0.0024, Prediction: aktab\n",
            "Epoch: 160/200, Loss: 0.0020, Prediction: aktab\n",
            "Epoch: 180/200, Loss: 0.0017, Prediction: aktab\n",
            "Epoch: 200/200, Loss: 0.0015, Prediction: aktab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4\n",
        "\n",
        "\n",
        "Modelning o‘rganish tahlili\n",
        "Olingan natijalar modelning \"maktab\" so‘zi ketma-ketligini to‘g‘ri bashorat qilishni o‘rganganligini ko‘rsatadi. Buni quyidagi ikki asosiy omil bilan asoslash mumkin:\n",
        "\n",
        "Yo‘qotish (Loss) qiymatining pasayishi: O‘qitishning ilk epoxalarida loss qiymati yuqori (masalan, 2.0 dan yuqori) bo‘ladi, chunki model harflar orasidagi bog‘liqlikni hali bilmaydi. Epoxalar soni ortib borishi bilan (ayniqsa 100-200 epoxalar oralig‘ida) loss qiymati barqaror ravishda kamayib, 0.1–0.5 atrofida yoki undan ham pastroq qiymatga tushadi. Bu modelning xatoligi kamayganidan dalolat beradi.\n",
        "\n",
        "Oxirgi bashorat (Pred) natijasi: 200-epoxaga kelib, model chiqaradigan pred_seq (oxirgi bashorat) natijasi \"aktab\" (yoki o‘qitish uchun tanlangan nishon so‘z) ko‘rinishiga keladi. Chunki biz x_data sifatida \"makta\" ni berganimizda, model har bir harfga mos ravishda keyingi harfni (\"a\", \"k\", \"t\", \"a\", \"b\") to‘g‘ri ketma-ketlikda chiqara boshlaydi.\n",
        "\n",
        "Xulosa\n",
        "Model \"maktab\" so‘zining harflar zanjirini o‘rgandi. RNN qatlami hidden state (yashirin holat) orqali avvalgi kelgan harfni eslab qolishni va u orqali keyingi ehtimoliy harfni topishni o‘zlashtirdi.\n",
        "\n",
        "Natija: Agar oxirgi epoxada Loss minimallashgan va Prediction qismida so‘z xatosiz (yoki deyarli xatosiz) chiqqan bo‘lsa, demak model berilgan vazifani muvaffaqiyatli bajargan."
      ],
      "metadata": {
        "id": "v9Z_KbJyBtmN"
      }
    }
  ]
}